{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2761dfb9-c19a-4191-90f2-f223390e2b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "from kafka.errors import KafkaError\n",
    "import json\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Load the JSON file\n",
    "json_file_path = \"../data/random_events.json\"\n",
    "\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for message in data:\n",
    "    future = producer.send('test-topic', message)\n",
    "\n",
    "    try:\n",
    "        record_metadata = future.get(timeout=10)\n",
    "        print(f'Message sent: {record_metadata.topic}, partition: {record_metadata.partition}, offset: {record_metadata.offset}')\n",
    "    except KafkaError as e:\n",
    "        print(f'Error sending message: {e}')\n",
    "\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4f45951-539a-4931-8e16-e407e495c09b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'test-topic', # the topic to consume messages from\n",
    "    bootstrap_servers=['localhost:9092'], # list of kafka brokers to connecto to\n",
    "    auto_offset_reset='earliest', # where to start reading messages when no offset is stored (earliest is from the beginning)\n",
    "    enable_auto_commit=True, # automatically commit offsets after consuming messages\n",
    "    value_deserializer=lambda x:  x.decode('utf-8') if x else None  # deserialize message values from bytes to utf-8 strings\n",
    ")\n",
    "\n",
    "aggregated_data = defaultdict(lambda: {\"likes\": 0, \"comments\": 0, \"shares\": 0})\n",
    "batch_size = 50\n",
    "batch_count = 0\n",
    "\n",
    "for message in consumer:\n",
    "    raw_value = message.value\n",
    "\n",
    "    if raw_value is None:\n",
    "        print(\"Received an empty message. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"Raw message: {raw_value}\")\n",
    "\n",
    "        event = json.loads(raw_value) if isinstance(raw_value, str) else raw_value\n",
    "\n",
    "\n",
    "        post_id = event.get(\"post_id\")\n",
    "        platform = event.get(\"platform\")\n",
    "        post_timestamp = event.get(\"timestamp\")\n",
    "        event_type = event.get(\"event_type\")\n",
    "\n",
    "        # Aggregate event counts\n",
    "        key = (post_id, post_timestamp, platform)\n",
    "\n",
    "        if event_type == \"like\":\n",
    "            aggregated_data[key][\"likes\"] += 1\n",
    "        elif event_type == \"comment\":\n",
    "            aggregated_data[key][\"comments\"] += 1\n",
    "        elif event_type == \"share\":\n",
    "            aggregated_data[key][\"shares\"] += 1\n",
    "\n",
    "        batch_count += 1\n",
    "\n",
    "        # Save to CSV in batches\n",
    "        if batch_count >= batch_size:\n",
    "            df = pd.DataFrame([\n",
    "                {\"post_id\": k[0], \"post_timestamp\": k[1], \"platform\": k[2], **v}\n",
    "                for k, v in aggregated_data.items()\n",
    "            ])\n",
    "\n",
    "            csv_file = \"social_media_engagement_bronze.csv\"\n",
    "            df.to_csv(csv_file, index=False)\n",
    "\n",
    "            print(f\"Updated CSV saved: {csv_file}\")\n",
    "\n",
    "            # Reset batch counter\n",
    "            batch_count = 0\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}, Raw message: {raw_value}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}, Raw message: {raw_value}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Kafka test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
